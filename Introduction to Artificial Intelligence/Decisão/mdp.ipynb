{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Introdução à Inteligência Artificial</font>\n",
    "\n",
    "## Probabilidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov decision processes (MDPs)\n",
    "\n",
    "Este notebook demonstra a criação de algoritmos de iteração de valor através de Processos de Decisão de markov (MDP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importando as classes\n",
    "from mdp import MDP, GridMDP, sequential_decision_environment, value_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisão\n",
    "\n",
    "Antes de começar a jogar com as implementações reais vamos analisar algumas coisas sobre MDPs.\n",
    "\n",
    "- Um processo estocástico tem a propriedade de ** Markov ** se a distribuição de probabilidade condicional de estados futuros do processo (condicional em estados passados e presentes) depende somente do estado presente, não da sequência de eventos que a precederam. Muitas vezes é possível modelar muitos fenômenos diferentes como um processo de Markov, sendo flexível com nossa definição de estado.\n",
    "   \n",
    "\n",
    "- Os MDPs nos ajudam a lidar com ambientes totalmente observáveis e não-determinísticos / estocásticos. Para lidar com casos parcialmente observáveis e estocásticos usamos a generalização de MDPs denominados POMDPs (processo de decisão de Markov parcialmente observável). Nosso objetivo geral para resolver um MDP é chegar a uma política que nos orienta a selecionar a melhor ação em cada estado, de modo a maximizar a soma esperada de recompensas futuras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP\n",
    "\n",
    "Para começar, vejamos a implementação da classe MDP definida em mdp.py A docstring nos diz o que é necessário para definir um MDP, ou seja - conjunto de estados, ações, estado inicial, modelo de transição e uma função de recompensa. Cada um deles é implementado como métodos. Não feche o pop-up para que você possa seguir ao longo da descrição do código abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualizando o código da classe\n",
    "%psource MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método ** _ _init_ _ ** incorpora os seguintes parâmetros:\n",
    "\n",
    "- init: o estado inicial.\n",
    "- actlist: Lista de ações possíveis em cada estado.\n",
    "- terminais: lista de estados de terminais onde apenas a ação possível é uma saída\n",
    "- gama: Fator de desconto. Isso garante que as recompensas atrasadas tenham menos valor em comparação com as imediatas.\n",
    "\n",
    "O método ** R ** retorna a recompensa para cada estado usando o self.reward dict.\n",
    "\n",
    "O método ** T ** não está implementado. Aqui retornamos (probabilidade, s ') pares onde s' pertence à lista de estado possível, tomando a a ação no estado s.\n",
    "\n",
    "O método ** actions ** retorna lista de ações possíveis em cada estado. Por padrão, retorna todas as ações para estados que não sejam estados terminais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos implementar o MDP simples na imagem abaixo. Os estados A, B têm ações X, Y disponíveis neles. Suas probabilidades são mostradas acima das setas. Começamos com o uso do MDP como classe base para o nosso CustomMDP. Obviamente, precisamos fazer algumas mudanças para se adequar ao nosso caso. Fazemos uso de uma matriz de transição como nossas transições não são muito simples.\n",
    "<img src=\"mdp-a.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Matriz de transição como dicionário aninhado. Estado -> Ações no estado -> Estados por cada ação -> Probabilidade\n",
    "t = {\n",
    "    \"A\": {\n",
    "            \"X\": {\"A\":0.3, \"B\":0.7},\n",
    "            \"Y\": {\"A\":1.0}\n",
    "         },\n",
    "    \"B\": {\n",
    "            \"X\": {\"End\":0.8, \"B\":0.2},\n",
    "            \"Y\": {\"A\":1.0}\n",
    "         },\n",
    "    \"End\": {}\n",
    "}\n",
    "\n",
    "init = \"A\"\n",
    "\n",
    "terminals = [\"End\"]\n",
    "\n",
    "rewards = {\n",
    "    \"A\": 5,\n",
    "    \"B\": -10,\n",
    "    \"End\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classe CustomMDP\n",
    "class CustomMDP(MDP):\n",
    "\n",
    "    def __init__(self, transition_matrix, rewards, terminals, init, gamma=.9):\n",
    "        # Todas as ações possíveis\n",
    "        actlist = []\n",
    "        for state in transition_matrix.keys():\n",
    "            actlist.extend(transition_matrix.keys())\n",
    "        actlist = list(set(actlist))\n",
    "\n",
    "        MDP.__init__(self, init, actlist, terminals=terminals, gamma=gamma)\n",
    "        self.t = transition_matrix\n",
    "        self.reward = rewards\n",
    "        for state in self.t:\n",
    "            self.states.add(state)\n",
    "\n",
    "    def T(self, state, action):\n",
    "        return [(new_state, prob) for new_state, prob in self.t[state][action].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finalmente nós instanciamos a classe com os parâmetros para o nosso MDP na imagem.\n",
    "our_mdp = CustomMDP(t, rewards, terminals, init, gamma=.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, nós representamos com sucesso nosso MDP. Mais tarde, vamos procurar maneiras de resolver este MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid MDP\n",
    "\n",
    "Agora olhamos para uma implementação concreta que faz uso do MDP como classe base. A classe GridMDP no módulo mdp é usada para representar um mundo de grade MDP. O código deve ser fácil de entender se você tiver passado pelo exemplo do CustomMDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%psource GridMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método ** _ _init__ ** toma ** grid ** como um parâmetro extra em comparação com a classe MDP e grid é uma lista aninhada de recompensas em estados.\n",
    "\n",
    "O método ** go ** retorna o estado indo em direção determinada usando vector_add.\n",
    "\n",
    "O método ** T ** não está implementado. Aqui retornamos (probabilidade, s ') pares onde s' pertence à lista de estado possível, tomando a a ação no estado s.\n",
    "\n",
    "O método ** actions ** retorna lista de ações possíveis em cada estado. Por padrão, retorna todas as ações para estados que não sejam estados terminais.\n",
    "\n",
    "O método ** to_arrows ** é usado para representar a política em uma grade como formato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos criar um GridMDP da seguinte forma:\n",
    "\n",
    "    GridMDP([[-0.04, -0.04, -0.04, +1],\n",
    "            [-0.04, None,  -0.04, -1],\n",
    "            [-0.04, -0.04, -0.04, -0.04]],\n",
    "            terminals=[(3, 2), (3, 1)])\n",
    "            \n",
    "Na verdade, o ** sequential_decision_environment ** no módulo mdp foi instanciado usando exatamente o mesmo código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mdp.GridMDP at 0x106b6fd68>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential_decision_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Iteração de Valor\n",
    "\n",
    "Agora que nós vimos como representar MDPs, vamos tentar resolvê-los. Nosso objetivo final é obter uma política ideal. Começamos com a análise da Iteração de Valores e uma visualização que deve nos ajudar a entendê-la melhor.\n",
    "\n",
    "Começamos por calcular Valor / Utilidade para cada um dos estados. O valor de cada estado é a soma esperada de recompensas futuras descontadas, dado que começamos nesse estado e seguimos uma política particular. O algoritmo Iteração de valor depende de encontrar soluções da equação de Bellman. A intuição Iteração de valores funciona porque os valores se propagam. Este ponto ficará mais claro com a visualização mais abaixo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%psource value_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toma como entradas dois parâmetros um MDP para resolver epsilon o erro máximo permitido na utilidade de qualquer estado. Ele retorna um dicionário contendo utilitários onde as chaves são os estados e os valores representam utilitários. Vamos resolver o ** sequencial_decision_enviornment ** GridMDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.2962883154554812,\n",
       " (0, 1): 0.3984432178350045,\n",
       " (0, 2): 0.5093943765842497,\n",
       " (1, 0): 0.25386699846479516,\n",
       " (1, 2): 0.649585681261095,\n",
       " (2, 0): 0.3447542300124158,\n",
       " (2, 1): 0.48644001739269643,\n",
       " (2, 2): 0.7953620878466678,\n",
       " (3, 0): 0.12987274656746342,\n",
       " (3, 1): -1.0,\n",
       " (3, 2): 1.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration(sequential_decision_environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização Para a Iteração de Valor\n",
    "\n",
    "Para ilustrar que os valores se propagam fora dos estados vamos criar uma visualização simples. Estaremos usando uma versão modificada da função value_iteration que armazenará U ao longo do tempo. Também removeremos o parâmetro epsilon e adicionaremos o número de iterações que queremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_iteration_instru(mdp, iterations=20):\n",
    "    U_over_time = []\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for _ in range(iterations):\n",
    "        U = U1.copy()\n",
    "        for s in mdp.states:\n",
    "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "        U_over_time.append(U)\n",
    "    return U_over_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, definimos uma função para criar a visualização a partir dos utilitários retornados por ** value_iteration_instru **. Abaixo usaremos o Matplotib com IPython Widgets. Se você estiver interessado em ler mais sobre esse recurso acesse [ipywidgets.readthedocs.io] (http://ipywidgets.readthedocs.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "def make_plot_grid_step_function(columns, row, U_over_time):\n",
    "    '''A função interativa ipywidgets suporta\n",
    "        um único parâmetro como entrada. Esta função\n",
    "        cria e devolve tal função, tomando\n",
    "        Na entrada outros parâmetros\n",
    "    '''\n",
    "    def plot_grid_step(iteration):\n",
    "        data = U_over_time[iteration]\n",
    "        data = defaultdict(lambda: 0, data)\n",
    "        grid = []\n",
    "        for row in range(rows):\n",
    "            current_row = []\n",
    "            for column in range(columns):\n",
    "                current_row.append(data[(column, row)])\n",
    "            grid.append(current_row)\n",
    "        grid.reverse() \n",
    "        fig = plt.imshow(grid, cmap=plt.cm.bwr, interpolation='nearest')\n",
    "\n",
    "        plt.axis('off')\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "        for col in range(len(grid)):\n",
    "            for row in range(len(grid[0])):\n",
    "                magic = grid[col][row]\n",
    "                fig.axes.text(row, col, \"{0:.2f}\".format(magic), va='center', ha='center')\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return plot_grid_step\n",
    "\n",
    "def make_visualize(slider):\n",
    "    ''' Toma uma entrada de um controle deslizante e retorna\n",
    "         Função callback para timer e animação\n",
    "    '''\n",
    "    \n",
    "    def visualize_callback(Visualize, time_step):\n",
    "        if Visualize is True:\n",
    "            for i in range(slider.min, slider.max + 1):\n",
    "                slider.value = i\n",
    "                time.sleep(float(time_step))\n",
    "    \n",
    "    return visualize_callback\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = 4\n",
    "rows = 3\n",
    "U_over_time = value_iteration_instru(sequential_decision_environment)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_grid_step = make_plot_grid_step_function(columns, rows, U_over_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clique no botão Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAFKCAYAAACThWFrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDaQtWucZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fe00978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "iteration_slider = widgets.IntSlider(min=1, max=15, step=1, value=0)\n",
    "w=widgets.interactive(plot_grid_step,iteration=iteration_slider)\n",
    "display(w)\n",
    "\n",
    "visualize_callback = make_visualize(iteration_slider)\n",
    "\n",
    "visualize_button = widgets.ToggleButton(desctiption = \"Visualize\", value = False)\n",
    "time_select = widgets.ToggleButtons(description='Extra Delay:',options=['0', '0.1', '0.2', '0.5', '0.7', '1.0'])\n",
    "a = widgets.interactive(visualize_callback, Visualize = visualize_button, time_step=time_select)\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mova o controle deslizante acima para observar como o utilitário é alterado nas iterações. Também é possível mover o controle deslizante usando as teclas de seta ou saltar para o valor editando diretamente o número com um duplo clique. O botão ** Visualize ** animará automaticamente o controle deslizante para você. O ** Extra Delay Box ** permite que você defina o atraso em segundos até um segundo para cada passo de tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIM"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "05f568803c334a3981ff495026956887": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "524770e71d7f40909971ecd76e903ed3": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "9bbaccf49f5d4e1cb16387646cf857c1": {
     "views": [
      {
       "cell_index": 28
      }
     ]
    },
    "d3a434d3a8a4407e9001789735ba9296": {
     "views": [
      {
       "cell_index": 27
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
