{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Deep Learning I</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historicamente, Perceptron foi o nome dado a um modelo de rede neural com uma única camada linear. Se o modelo tem múltiplas camadas, chamamos de Perceptron Multicamada (MLP - Multilayer Perceprtron). Cada nó na primeira camada recebe uma entrada e dispara de acordo com os limites de decisão locais predefinidos (thresholds). Em seguida, a saída da primeira camada é passada para a segunda camada, cujos resultados são passados para a camada de saída final consistindo de um único neurônio. \n",
    "\n",
    "A rede pode ser densa, o que significa que cada neurônio em uma camada está conectado a todos os neurônios localizados na camada anterior e a todos os neurônios na camada seguinte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando Redes Neurais com Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos considerar um único neurônio. Quais são as melhores escolhas para o peso w e o bias b? Idealmente, gostaríamos de fornecer um conjunto de exemplos de treinamento e deixar o computador ajustar o peso e o bias de tal forma que os erros produzidos na saída sejam minimizados. A fim de tornar isso um pouco mais concreto, vamos supor que temos um conjunto de imagens de gatos e outro conjunto separado de imagens que não contenham gatos. Por uma questão de simplicidade, suponha que cada neurônio olhe para um único valor de pixel de entrada. Enquanto o computador processa essas imagens, gostaríamos que nosso neurônio ajustasse seus pesos e bias para que tenhamos menos e menos imagens erroneamente reconhecidas como não-gatos. Essa abordagem parece muito intuitiva, mas exige que uma pequena alteração nos pesos (e/ou bias) cause apenas uma pequena mudança nas saídas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tivermos um grande salto de saída, não podemos aprender progressivamente. Afinal, as crianças aprendem pouco a pouco. Infelizmente, o perceptron não mostra esse comportamento \"pouco a pouco\". Um perceptron é 0 ou 1 e isso é um grande salto e não vai ajudá-lo a aprender. Precisamos de algo diferente, mais suave. Precisamos de uma função que mude progressivamente de 0 para 1 sem descontinuidade. Matematicamente, isso significa que precisamos de uma função contínua que nos permita calcular a derivada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada neurônio pode ser inicializado com pesos específicos. Keras oferece algumas opções, a mais comum são:\n",
    "\n",
    "Random_uniform: Os pesos são inicializados com valores uniformemente pequenos e aleatórios em (-0,05, 0,05). \n",
    "\n",
    "Random_normal: Os pesos são inicializados de acordo com uma distribuição Gaussiana, com média zero e pequeno desvio padrão de 0,05. \n",
    "\n",
    "Zero: Todos os pesos são inicializados para zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras\n",
    "# !pip install tensorflow\n",
    "# !pip install scikit-learn\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.17.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/ml/datasets/pima+indians+diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o dataset\n",
    "#dataset = numpy.loadtxt(\"/media/datasets/DeepLearningI/Cap04/data.csv\", delimiter=\",\")\n",
    "dataset = numpy.loadtxt(\"data.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprime o dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split em variáveis de input (X) e output (Y) \n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into 67% for train and 33% for test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/initializers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim = 8, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(8, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural network 1 hidden layer](01-arquitetura-rede-neural.png \"Rede Neural com 1 Camada Oculta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função Sigmóide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função sigmóide é uma função matemática de amplo uso em campos como a economia e a computação. O nome \"sigmóide\" vem da forma em S do seu gráfico. Um neurônio pode usar o sigmóide para calcular a função não-linear. Um neurônio com ativação sigmóide tem um comportamento semelhante ao perceptron, mas as mudanças são graduais e os valores de saída, como 0.3537 ou 0.147191, são perfeitamente legítimos. A função de ativação sigmóide é comumente utilizada por redes neurais com propagação positiva (Feedforward) que precisam ter como saída apenas números positivos, em redes neurais multicamadas e em outras redes com sinais contínuos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função ReLu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O sigmóide não é o único tipo de função de ativação suave usada para redes neurais. Recentemente, uma função muito simples chamada unidade linear rectificada (ReLU) tornou-se muito popular porque gera resultados experimentais muito bons. Uma ReLU é simplesmente definida como uma função não-linear e a função é zero para valores negativos e cresce linearmente para valores positivos.\n",
    "Sigmoid e ReLU são geralmente chamados funções de ativação das redes neurais. Essas mudanças graduais, típicas das funções Sigmóide e ReLU, são os blocos básicos para o desenvolvimento de um algoritmo de aprendizado que se adapta pouco a pouco, reduzindo progressivamente os erros cometidos pelas redes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilação do modelo\n",
    "# Precisamos selecionar o otimizador que é o algoritmo específico usado para atualizar pesos enquanto \n",
    "# treinamos nosso modelo.\n",
    "# Precisamos selecionar também a função objetivo que é usada pelo otimizador para navegar no espaço de pesos \n",
    "# (frequentemente, as funções objetivo são chamadas de função de perda (loss) e o processo de otimização é definido \n",
    "# como um processo de minimização de perdas).\n",
    "# Outras funções aqui: https://keras.io/losses/\n",
    "# A função objetivo \"categorical_crossentropy\" é a função objetivo adequada para predições de rótulos multiclass e \n",
    "# binary_crossentropy para classificação binária. \n",
    "# A métrica é usada para medir a performance do modelo. Outras métricas: https://keras.io/metrics/\n",
    "# As métricas são semelhantes às funções objetivo, com a única diferença de que elas não são usadas para \n",
    "# treinar um modelo, mas apenas para avaliar um modelo. \n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "514/514 [==============================] - 0s 302us/step - loss: 0.6792 - accuracy: 0.6576\n",
      "Epoch 2/150\n",
      "514/514 [==============================] - 0s 83us/step - loss: 0.6642 - accuracy: 0.6576\n",
      "Epoch 3/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.6583 - accuracy: 0.6576\n",
      "Epoch 4/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.6535 - accuracy: 0.6595\n",
      "Epoch 5/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.6503 - accuracy: 0.6654\n",
      "Epoch 6/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.6447 - accuracy: 0.6556\n",
      "Epoch 7/150\n",
      "514/514 [==============================] - 0s 68us/step - loss: 0.6328 - accuracy: 0.6712\n",
      "Epoch 8/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.6254 - accuracy: 0.6732\n",
      "Epoch 9/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.6155 - accuracy: 0.6809\n",
      "Epoch 10/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.6065 - accuracy: 0.6907\n",
      "Epoch 11/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.6013 - accuracy: 0.6984\n",
      "Epoch 12/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.6007 - accuracy: 0.6907\n",
      "Epoch 13/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.5985 - accuracy: 0.7082\n",
      "Epoch 14/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.5855 - accuracy: 0.7043\n",
      "Epoch 15/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.5886 - accuracy: 0.7043\n",
      "Epoch 16/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.5875 - accuracy: 0.7023\n",
      "Epoch 17/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.5791 - accuracy: 0.7218\n",
      "Epoch 18/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.5748 - accuracy: 0.7198\n",
      "Epoch 19/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.5737 - accuracy: 0.7218\n",
      "Epoch 20/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.5733 - accuracy: 0.7121\n",
      "Epoch 21/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.5668 - accuracy: 0.7257\n",
      "Epoch 22/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.5627 - accuracy: 0.7335\n",
      "Epoch 23/150\n",
      "514/514 [==============================] - 0s 77us/step - loss: 0.5691 - accuracy: 0.7257\n",
      "Epoch 24/150\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.5666 - accuracy: 0.7140\n",
      "Epoch 25/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.5632 - accuracy: 0.7160\n",
      "Epoch 26/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.5601 - accuracy: 0.7354\n",
      "Epoch 27/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.5587 - accuracy: 0.7218\n",
      "Epoch 28/150\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.5571 - accuracy: 0.7257\n",
      "Epoch 29/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.5532 - accuracy: 0.7218\n",
      "Epoch 30/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.5628 - accuracy: 0.7198\n",
      "Epoch 31/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.5547 - accuracy: 0.7257\n",
      "Epoch 32/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.5456 - accuracy: 0.7315\n",
      "Epoch 33/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.5474 - accuracy: 0.7432\n",
      "Epoch 34/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.5478 - accuracy: 0.7315\n",
      "Epoch 35/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.5449 - accuracy: 0.7315\n",
      "Epoch 36/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.5390 - accuracy: 0.7335\n",
      "Epoch 37/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.5364 - accuracy: 0.7315\n",
      "Epoch 38/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.5425 - accuracy: 0.7237\n",
      "Epoch 39/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.5401 - accuracy: 0.7296\n",
      "Epoch 40/150\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.5394 - accuracy: 0.7393\n",
      "Epoch 41/150\n",
      "514/514 [==============================] - 0s 76us/step - loss: 0.5373 - accuracy: 0.7296\n",
      "Epoch 42/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.5377 - accuracy: 0.7237\n",
      "Epoch 43/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.5361 - accuracy: 0.7335\n",
      "Epoch 44/150\n",
      "514/514 [==============================] - 0s 78us/step - loss: 0.5482 - accuracy: 0.7198\n",
      "Epoch 45/150\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.5362 - accuracy: 0.7393\n",
      "Epoch 46/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.5278 - accuracy: 0.7354\n",
      "Epoch 47/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.5340 - accuracy: 0.7296\n",
      "Epoch 48/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.5326 - accuracy: 0.7374\n",
      "Epoch 49/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.5271 - accuracy: 0.7393\n",
      "Epoch 50/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.5239 - accuracy: 0.7257\n",
      "Epoch 51/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.5260 - accuracy: 0.7276\n",
      "Epoch 52/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.5178 - accuracy: 0.7451\n",
      "Epoch 53/150\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.5291 - accuracy: 0.7335\n",
      "Epoch 54/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.5507 - accuracy: 0.7121\n",
      "Epoch 55/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.5269 - accuracy: 0.7335\n",
      "Epoch 56/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.5277 - accuracy: 0.7412\n",
      "Epoch 57/150\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.5296 - accuracy: 0.7237\n",
      "Epoch 58/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.5175 - accuracy: 0.7549\n",
      "Epoch 59/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.5154 - accuracy: 0.7471\n",
      "Epoch 60/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.5208 - accuracy: 0.7374\n",
      "Epoch 61/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.5187 - accuracy: 0.7335\n",
      "Epoch 62/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.5115 - accuracy: 0.7529\n",
      "Epoch 63/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.5247 - accuracy: 0.7626\n",
      "Epoch 64/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.5112 - accuracy: 0.7412\n",
      "Epoch 65/150\n",
      "514/514 [==============================] - 0s 76us/step - loss: 0.5151 - accuracy: 0.7510\n",
      "Epoch 66/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.5282 - accuracy: 0.7315\n",
      "Epoch 67/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.5142 - accuracy: 0.7315\n",
      "Epoch 68/150\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.5117 - accuracy: 0.7315\n",
      "Epoch 69/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.5093 - accuracy: 0.7471\n",
      "Epoch 70/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.5080 - accuracy: 0.7412\n",
      "Epoch 71/150\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.5019 - accuracy: 0.7549\n",
      "Epoch 72/150\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.5192 - accuracy: 0.7393\n",
      "Epoch 73/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.4986 - accuracy: 0.7529\n",
      "Epoch 74/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.5030 - accuracy: 0.7510\n",
      "Epoch 75/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.5092 - accuracy: 0.7646\n",
      "Epoch 76/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.5049 - accuracy: 0.7451\n",
      "Epoch 77/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.5045 - accuracy: 0.7588\n",
      "Epoch 78/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.5056 - accuracy: 0.7490\n",
      "Epoch 79/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.4964 - accuracy: 0.7646\n",
      "Epoch 80/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514/514 [==============================] - 0s 74us/step - loss: 0.5007 - accuracy: 0.7510\n",
      "Epoch 81/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.4968 - accuracy: 0.7549\n",
      "Epoch 82/150\n",
      "514/514 [==============================] - 0s 68us/step - loss: 0.5049 - accuracy: 0.7451\n",
      "Epoch 83/150\n",
      "514/514 [==============================] - 0s 68us/step - loss: 0.5086 - accuracy: 0.7471\n",
      "Epoch 84/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.5110 - accuracy: 0.7393\n",
      "Epoch 85/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4987 - accuracy: 0.7607\n",
      "Epoch 86/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4934 - accuracy: 0.7724\n",
      "Epoch 87/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4903 - accuracy: 0.7510\n",
      "Epoch 88/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.4909 - accuracy: 0.7568\n",
      "Epoch 89/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4931 - accuracy: 0.7568\n",
      "Epoch 90/150\n",
      "514/514 [==============================] - 0s 68us/step - loss: 0.4880 - accuracy: 0.7588\n",
      "Epoch 91/150\n",
      "514/514 [==============================] - 0s 68us/step - loss: 0.4899 - accuracy: 0.7568\n",
      "Epoch 92/150\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.4854 - accuracy: 0.7568\n",
      "Epoch 93/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4926 - accuracy: 0.7549\n",
      "Epoch 94/150\n",
      "514/514 [==============================] - 0s 74us/step - loss: 0.4896 - accuracy: 0.7471\n",
      "Epoch 95/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.4828 - accuracy: 0.7471\n",
      "Epoch 96/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4896 - accuracy: 0.7510\n",
      "Epoch 97/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4790 - accuracy: 0.7607\n",
      "Epoch 98/150\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.4773 - accuracy: 0.7588\n",
      "Epoch 99/150\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.4797 - accuracy: 0.7665\n",
      "Epoch 100/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4913 - accuracy: 0.7646\n",
      "Epoch 101/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4819 - accuracy: 0.7568\n",
      "Epoch 102/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4895 - accuracy: 0.7549\n",
      "Epoch 103/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4808 - accuracy: 0.7743\n",
      "Epoch 104/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4785 - accuracy: 0.7549\n",
      "Epoch 105/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4806 - accuracy: 0.7588\n",
      "Epoch 106/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4774 - accuracy: 0.7568\n",
      "Epoch 107/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4804 - accuracy: 0.7510\n",
      "Epoch 108/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.4829 - accuracy: 0.7607\n",
      "Epoch 109/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.4855 - accuracy: 0.7529\n",
      "Epoch 110/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4835 - accuracy: 0.7626\n",
      "Epoch 111/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.4729 - accuracy: 0.7588\n",
      "Epoch 112/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4738 - accuracy: 0.7665\n",
      "Epoch 113/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4736 - accuracy: 0.7607\n",
      "Epoch 114/150\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.4836 - accuracy: 0.7529\n",
      "Epoch 115/150\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.4774 - accuracy: 0.7529\n",
      "Epoch 116/150\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.4812 - accuracy: 0.7626\n",
      "Epoch 117/150\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.4694 - accuracy: 0.7724\n",
      "Epoch 118/150\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.4776 - accuracy: 0.7529\n",
      "Epoch 119/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4724 - accuracy: 0.7588\n",
      "Epoch 120/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4710 - accuracy: 0.7626\n",
      "Epoch 121/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4672 - accuracy: 0.7704\n",
      "Epoch 122/150\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.4672 - accuracy: 0.7860\n",
      "Epoch 123/150\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.4730 - accuracy: 0.7549\n",
      "Epoch 124/150\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.4626 - accuracy: 0.7685\n",
      "Epoch 125/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4727 - accuracy: 0.7665\n",
      "Epoch 126/150\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.4646 - accuracy: 0.7704\n",
      "Epoch 127/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4660 - accuracy: 0.7646\n",
      "Epoch 128/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.4655 - accuracy: 0.7685\n",
      "Epoch 129/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.4623 - accuracy: 0.7588\n",
      "Epoch 130/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4680 - accuracy: 0.7665\n",
      "Epoch 131/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4645 - accuracy: 0.7685\n",
      "Epoch 132/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4622 - accuracy: 0.7665\n",
      "Epoch 133/150\n",
      "514/514 [==============================] - 0s 73us/step - loss: 0.4730 - accuracy: 0.7821\n",
      "Epoch 134/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4625 - accuracy: 0.7607\n",
      "Epoch 135/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.4584 - accuracy: 0.7588\n",
      "Epoch 136/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.4601 - accuracy: 0.7665\n",
      "Epoch 137/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4713 - accuracy: 0.7685\n",
      "Epoch 138/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4581 - accuracy: 0.7743\n",
      "Epoch 139/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.4683 - accuracy: 0.7782\n",
      "Epoch 140/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4694 - accuracy: 0.7685\n",
      "Epoch 141/150\n",
      "514/514 [==============================] - 0s 68us/step - loss: 0.4606 - accuracy: 0.7685\n",
      "Epoch 142/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4656 - accuracy: 0.7529\n",
      "Epoch 143/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4628 - accuracy: 0.7665\n",
      "Epoch 144/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4529 - accuracy: 0.7646\n",
      "Epoch 145/150\n",
      "514/514 [==============================] - 0s 67us/step - loss: 0.4550 - accuracy: 0.7782\n",
      "Epoch 146/150\n",
      "514/514 [==============================] - 0s 70us/step - loss: 0.4562 - accuracy: 0.7704\n",
      "Epoch 147/150\n",
      "514/514 [==============================] - 0s 75us/step - loss: 0.4544 - accuracy: 0.7646\n",
      "Epoch 148/150\n",
      "514/514 [==============================] - 0s 72us/step - loss: 0.4534 - accuracy: 0.7763\n",
      "Epoch 149/150\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.4574 - accuracy: 0.7724\n",
      "Epoch 150/150\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.4521 - accuracy: 0.7743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb1aa052250>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinamento do modelo\n",
    "# Epochs: Este é o número de vezes que o modelo é exposto ao conjunto de treinamento. Em cada iteração, \n",
    "# o otimizador tenta ajustar os pesos para que a função objetivo seja minimizada. \n",
    "# Batch_size: Esse é o número de instâncias de treinamento observadas antes que o otimizador execute uma \n",
    "# atualização de peso.\n",
    "model.fit(X_train, y_train, epochs = 150, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 0s 87us/step\n",
      "\n",
      "Loss: 0.55, Acurácia: 75.20%\n"
     ]
    }
   ],
   "source": [
    "# Avalia o modelo com os dados de teste\n",
    "# Uma vez treinado o modelo, podemos avaliá-lo no conjunto de testes que contém novos exemplos não vistos. \n",
    "# Desta forma, podemos obter o valor mínimo alcançado pela função objetivo e o melhor valor alcançado pela métrica \n",
    "# de avaliação. Note-se que o conjunto de treinamento e o conjunto de teste são rigorosamente separados. \n",
    "# Não vale a pena avaliar um modelo em um exemplo que já foi usado para treinamento. \n",
    "# A aprendizagem é essencialmente um processo destinado a generalizar observações invisíveis e não a memorizar \n",
    "# o que já é conhecido.\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"\\nLoss: %.2f, Acurácia: %.2f%%\" % (loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera as previsões\n",
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0]\n",
      "Acurácia das Previsões: 76.95%\n"
     ]
    }
   ],
   "source": [
    "# Ajusta as previsões e imprime o resultado\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)\n",
    "accuracy = numpy.mean(rounded == Y)\n",
    "print(\"Acurácia das Previsões: %.2f%%\" % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
