{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Deep Learning Frameworks</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.8.1\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab - Batch Normalization com MXNet\n",
    "\n",
    "Em Machine Learning em geral, é muito comum normalizarmos a camada de entrada ajustando e dimensionando os dados. \n",
    "\n",
    "Por exemplo, quando temos atributos (variáveis) com valores de 0 a 1 e alguns de 1 a 1000, devemos normalizá-los para acelerar o aprendizado (colocar todos os dados na mesma escala). Se a camada de entrada está se beneficiando da normalização, por que não fazer o mesmo com os valores nas camadas ocultas, que mudam o tempo todo? Ao fazer isso, poderíamos obter 10 vezes ou mais melhorias na velocidade de treinamento.\n",
    "\n",
    "A normalização em lotes reduz a quantidade pela qual os valores das unidades ocultas mudam (mudança de covariância). Para explicar a mudança de covariância, considere como exemplo uma rede profunda para detecção de gatos. Treinamos nossos dados apenas nas imagens de gatos pretos. Portanto, se agora tentarmos aplicar essa rede a dados com gatos coloridos, não vamos nos sair bem. O conjunto de treinamento e o conjunto de previsão são imagens de gatos, mas diferem um pouco. Em outras palavras, se um algoritmo aprendeu algum mapeamento de X para Y e se a distribuição de X mudar, talvez seja necessário treinar novamente o modelo, tentando alinhar a distribuição de X com a distribuição de Y.\n",
    "\n",
    "Além disso, a normalização em lote permite que cada camada de uma rede aprenda sozinha um pouco mais independentemente de outras camadas.\n",
    "\n",
    "Ao usar Batch Normalization, podemos usar taxas de aprendizado mais altas porque a normalização em lote garante que não haja ativação muito alta ou muito baixa. \n",
    "\n",
    "Batch Normalization também reduz a adaptação excessiva (overfitting), porque apresenta alguns efeitos de regularização. Semelhante ao Dropout, essa técnica adiciona algum ruído às ativações de cada camada oculta. Portanto, se usarmos a normalização em lote, usaremos menos dropouts, o que é uma coisa boa, pois não perderemos muitas informações. No entanto, não devemos depender apenas da normalização de lotes para regularização; devemos usá-lo em conjunto com o Dropout.\n",
    "\n",
    "É o que faremos agora neste Lab.\n",
    "\n",
    "Referências:\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1502.03167v3.pdf\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>\n",
    "\n",
    "<a href=\"http://www.deeplearningbook.com.br/\">Deep Learning Book</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec  2 04:23:59 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 462.31       Driver Version: 462.31       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce MX150      WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   57C    P8    N/A /  N/A |     68MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\users\\rapha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\users\\rapha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\users\\rapha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\users\\rapha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\users\\rapha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\users\\rapha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'd:\\users\\rapha\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\users\\rapha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\users\\rapha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement mxnet-cu102 (from versions: none)\n",
      "ERROR: No matching distribution found for mxnet-cu102\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\users\\rapha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\users\\rapha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\users\\rapha\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'd:\\users\\rapha\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Instala o pacote MXNet com suporte a GPU\n",
    "!pip install -q mxnet-cu102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sAxdD0b_0njT"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mxnet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5a9aa74c5487>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmxnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautograd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgluon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmxnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgluon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mxnet'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon import nn, data\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faremos o treinamento em GPU\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6lgNphf1AIV"
   },
   "source": [
    "### Carregando o Dataset\n",
    "\n",
    "Criaremos um modelo de Deep Learning para classificação de imagens usando o dataset CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "v2Tbe1PF0_q8",
    "outputId": "49d5d523-a0f4-4e92-ca8c-629f54854e32"
   },
   "outputs": [],
   "source": [
    "# Função para transformar os dados\n",
    "def transform(data, label):\n",
    "    return nd.moveaxis(data.astype('float32'), 2, 0)/255.0, label.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os dados de treino\n",
    "cifar_treino = data.vision.datasets.CIFAR10(train = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os dados de teste\n",
    "cifar_teste = data.vision.datasets.CIFAR10(train = False, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VahNidgo2Bsh",
    "outputId": "faf1a8c3-4057-4c49-faee-10881ca0855f"
   },
   "outputs": [],
   "source": [
    "# Extrai uma imagem\n",
    "image, label = cifar_treino[456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape da imagem\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape do label\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo o DataLoader\n",
    "\n",
    "O DataLoader extrai batches de dados para alimentar o modelo durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0vmp1W61ZG0"
   },
   "outputs": [],
   "source": [
    "# Tamanho do batch\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os batches de treino e teste\n",
    "dados_treino = data.DataLoader(cifar_treino, batch_size, True)\n",
    "dados_teste = data.DataLoader(cifar_teste, batch_size, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wjhB-e6X19jo"
   },
   "source": [
    "### Construindo o Modelo\n",
    "\n",
    "Agora construímos o modelo de rede neural para classificação de imagens usando Batch Normalization.\n",
    "\n",
    "Observe que o Batch Normalization deve vir antes do Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IicgDZNB1ZDa"
   },
   "outputs": [],
   "source": [
    "# Cria o modelo\n",
    "model = nn.Sequential()\n",
    "\n",
    "# Primeira camada convolucional com Batch Normalization\n",
    "model.add(nn.Conv2D(channels = 16, kernel_size = (5, 5)))\n",
    "model.add(nn.BatchNorm())\n",
    "model.add(nn.Activation('relu'))\n",
    "model.add(nn.MaxPool2D(pool_size = 2, strides = 1))\n",
    "\n",
    "# Segunda camada convolucional com Batch Normalization\n",
    "model.add(nn.Conv2D(channels = 32, kernel_size = (5, 5)))\n",
    "model.add(nn.BatchNorm())\n",
    "model.add(nn.Activation('relu'))\n",
    "model.add(nn.MaxPool2D(pool_size = 2, strides = 1))\n",
    "\n",
    "# Terceira camada convolucional com Batch Normalization\n",
    "model.add(nn.Conv2D(channels = 64, kernel_size = (5, 5)))\n",
    "model.add(nn.BatchNorm())\n",
    "model.add(nn.Activation('relu'))\n",
    "model.add(nn.MaxPool2D(pool_size = 2, strides = 1))\n",
    "\n",
    "# Flatten\n",
    "model.add(nn.Flatten())\n",
    "\n",
    "# Camada Densa com Batch Normalization\n",
    "model.add(nn.Dense(512))\n",
    "model.add(nn.BatchNorm())\n",
    "model.add(nn.Activation('relu'))\n",
    "model.add(nn.Dropout(0.5))\n",
    "\n",
    "# Camada Densa com Batch Normalization\n",
    "model.add(nn.Dense(256))\n",
    "model.add(nn.BatchNorm())\n",
    "model.add(nn.Activation('relu'))\n",
    "model.add(nn.Dropout(0.5))\n",
    "\n",
    "# Camada de saída\n",
    "model.add(nn.Dense(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa os hiperpaâmetros\n",
    "model.initialize(mx.init.Xavier(), ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "pWiLWyuZ9zJm",
    "outputId": "dd1d3ff8-cbfd-46fd-cf7d-fb99cc30508a"
   },
   "outputs": [],
   "source": [
    "# Modelo criado\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3joU6JO4cpG"
   },
   "source": [
    "Vamos verificar um sumário completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "colab_type": "code",
    "id": "WRRnW6fL1ZAx",
    "outputId": "6a86da00-ea99-43ad-b5be-b3353b3c7d08"
   },
   "outputs": [],
   "source": [
    "# Sumário do modelo\n",
    "model.summary(image.expand_dims(0).as_in_context(ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-WJ5sER5BeG"
   },
   "source": [
    "Precisamos de uma função de custo e usaremos a SoftmaxCrossEntropyLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-aOLi1X55Apc"
   },
   "outputs": [],
   "source": [
    "# Função de custo\n",
    "objective = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzhEgsFk5Ps6"
   },
   "source": [
    "E também precisamos de um otimizador para atualizar os pesos a cada passada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mq7S_Vws5Amj"
   },
   "outputs": [],
   "source": [
    "# Otimizador\n",
    "optimizer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': 0.001})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbHoiGM3ZrQY"
   },
   "source": [
    "Por fim, usaremos a acurácia como métrica de avaliação do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3osTW97ZuHR"
   },
   "outputs": [],
   "source": [
    "# Métrica do modelo\n",
    "metric = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UlkEn5XxFPkd"
   },
   "source": [
    "### Treinamento do Modelo\n",
    "\n",
    "Agora treinamos o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de épocas e listas para erros e acurácias em cada época\n",
    "epochs = 10\n",
    "losses = []\n",
    "accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "kVao6lbWCizF",
    "outputId": "0b041060-0a9a-4d2d-f04a-f00531eefc55"
   },
   "outputs": [],
   "source": [
    "# Loop de treinamento\n",
    "\n",
    "print(\"\\nIniciando o Treinamento...\\n\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Inicializa o erro acumulado\n",
    "    cumulative_loss = 0\n",
    "    \n",
    "    # Reset da métrica\n",
    "    metric.reset()\n",
    "    \n",
    "    # Loop pelos batches de dados\n",
    "    for batches, (features, labels) in enumerate(dados_treino, 1):\n",
    "        \n",
    "        # Envia dados de entrada e saída para a GPU\n",
    "        features = features.as_in_context(ctx)\n",
    "        labels = labels.as_in_context(ctx)\n",
    "        \n",
    "        # Executa a previsão do modelo e calcula o erro\n",
    "        with autograd.record():\n",
    "            output = model(features)\n",
    "            loss = objective(output, labels)\n",
    "            \n",
    "        # Inicia o backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Atualiza os pesos para a próxima passada de treino\n",
    "        optimizer.step(batch_size)\n",
    "        \n",
    "        # Acumula o erro médio\n",
    "        cumulative_loss += loss.mean()\n",
    "        \n",
    "        # Calcula a métrica\n",
    "        metric.update(labels, output)\n",
    "        \n",
    "    # Extrai a acurácia    \n",
    "    acc = metric.get()[1]\n",
    "    \n",
    "    # Alimenta as listas de erro e acurácias\n",
    "    losses.append(cumulative_loss.asscalar())\n",
    "    accs.append(acc)\n",
    "    \n",
    "    # print\n",
    "    print(f'Epoch: {epoch} | Erro: {cumulative_loss.asscalar()/(batches):.5f} | Acurácia: {acc:.5f}')\n",
    "\n",
    "print(\"\\nTreinamento Concluído.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeramos a métrica\n",
    "metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uX381SKr5aUW",
    "outputId": "32bf310b-d7a6-4dd8-e1b4-e457c4392d4f"
   },
   "outputs": [],
   "source": [
    "# Loop para previsões nos dados de teste\n",
    "for features, labels in dados_teste:\n",
    "    features = features.as_in_context(mx.gpu())\n",
    "    labels = labels.as_in_context(mx.gpu())\n",
    "    predictions = model(features)\n",
    "    metric.update(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Acurácia em Teste: {metric.get()[1]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "Fa9tf4eolx9r",
    "outputId": "66987073-9ffb-4551-af16-2f7dc91eff88"
   },
   "outputs": [],
   "source": [
    "# Plots\n",
    "plt.plot(accs, c = 'g')\n",
    "plt.title('Acurácia em Treino')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(losses, c = 'r')\n",
    "plt.title('Erro em Treino')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization podem ser uma boa opção quando precisarmos aumentar a precisão do nosso modelo, reduzindo o risco de overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Batch Normalization - MXNet (Using Gluon).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
